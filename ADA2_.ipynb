{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba36903-ee71-4a62-9bb8-8e0270fdae64",
   "metadata": {},
   "source": [
    "# Advanced Text Analytics Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244c460-fa21-482a-a6e6-4216f8f74c24",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook is the second of two lab notebooks that you will submit as part of your assessment for the Advanced Data Analytics unit. The notebook contains three required sections plus an optional section:\n",
    "\n",
    "4. **Introducing Transformers:** This section introduces the Transformers library from HuggingFace, showing you how to use it to obtain contextualised embeddings from pretrained transformer models.\n",
    "\n",
    "5. **Question Answering with Pretrained Transformers:** Learn about how to use a pretrained model to perform automatic question answering. \n",
    "\n",
    "6. **Transformers for Text Classification:** Here we show you how to construct a classifier using Transformers.\n",
    "\n",
    "7. **OPTIONAL: More on Transformers:** Some pointers to other materials if you want to learn more about transformers, e.g., if using them in your summer project. \n",
    "\n",
    "Example code for all the tasks has been tested on a three-year old MacBook Pro, and the longest training process took under 10 minutes. If you find that the code takes too long to run on your own machine, you can try [Google Colab](https://colab.research.google.com/), Amazon Sagemaker Studio, or use lab machines on campus provided by the school. \n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "These sections will contain tutorial-like instructions, as you have seen in previous text analytics labs. On completing these sections, the intended learning outcomes are that you will be able to...\n",
    "1. Use pretrained transformers to obtain contextualised word and sentence embeddings.\n",
    "1. Apply a pretrained QA model to a new dataset. \n",
    "1. Construct classifiers with pretrained transformers. \n",
    "1. Find documentation on pretrained models in the Transformers library.\n",
    "\n",
    "## Your Tasks\n",
    "\n",
    "Inside each of these sections there are several **'To-do's**, which you must complete for your summative assessment. Your marks will be based on your answers to these to-dos. Please make sure to:\n",
    "1. Include the output of your code in the saved notebook. Plots and printed output should be visible without re-running the code. \n",
    "1. Include all code needed to generate your answers.\n",
    "1. Provide sufficient comments to understand how your method works.\n",
    "1. Write text in a cell in markdown format where a written answer is required. You can convert a cell to markdown format by pressing Escape-M. \n",
    "\n",
    "There are also some unmarked 'to-do's that are part of the tutorial to help you learn how to implement and use the methods studied here.\n",
    "\n",
    "## Good Academic Practice\n",
    "\n",
    "Please follow [the guidance on academic integrity provided by the university](http://www.bristol.ac.uk/students/support/academic-advice/academic-integrity/).\n",
    "You are required to write your own answers -- do not share your notebooks or copy someone else's writing. Do not copy text or long blocks of code directly into the notebook from online sources -- always rewrite in your own way. Breaking the rules can lead to strong penalties. \n",
    "\n",
    "## Marking Criteria\n",
    "\n",
    "1. The coursework (both notebooks) is worth 30% of the unit in total. \n",
    "1. There is a total of 100 marks available for both lab notebooks. \n",
    "1. This notebook is worth 50 of those marks.\n",
    "1. The number of marks for each to-do out of 100 is shown alongside each to-do.\n",
    "1. For to-dos that require you to write code, a good solution would meet the following criteria (in order of importance):\n",
    "   1. Solves the task or answers the question asked in the to-do. This means, if the code cells in the notebook are executed in order, we will get the output shown in your notebook.\n",
    "   1. The code is easy to follow and does not contain unnecessary steps.\n",
    "   1. The comments show that you understand how your solution works.\n",
    "   1. A very good answer will also provide code that is computationally efficient but easy to read.\n",
    "1. You can use any suitable publicly available libraries. Unless the task explicitly asks you to implement something from scratch, there is no penalty for using libraries to implement some steps.\n",
    "\n",
    "## Support\n",
    "\n",
    "The main source of support will be during the remaining lab sessions (Fridays 3-6pm) for this unit. \n",
    "\n",
    "The TAs and lecturer will help you with questions about the lectures, the code provided for you in this notebook, and general questions about the topics we cover. For the marked 'to-dos', they can only answer clarifying questions about what you have to do. \n",
    "\n",
    "Office hours: You can book office hours with Edwin on Mondays 3pm-5pm by sending him an email (edwin.simpson@bristol.ac.uk). If those times are not possible for you, please contact him by email to request an alternative. \n",
    "\n",
    "## Deadline\n",
    "\n",
    "The notebook must be submitted along with the second notebook on Blackboard before **Wednesday 24th May at 13.00**. \n",
    "\n",
    "## Submission\n",
    "\n",
    "You will need to zip up this notebook with the previous notebook into a single .zip file, which you will submit to Blackboard through the 'assessment, submission and feedback' link on the left sidebar. \n",
    "\n",
    "Please name your files like this:\n",
    "   * Name this notebook ADA2_<student_number>.ipynb\n",
    "   * Name the zip file <student_number>.zip\n",
    "   * Please don't use your name anywhere as we want to mark anonymously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3b707e-0ce9-49cb-b226-6ad387fddebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a9045-9d0b-4427-b91c-a085496de2a5",
   "metadata": {},
   "source": [
    "# 4. Pretrained Transformers (max. 15 marks)\n",
    "\n",
    "HuggingFace is a company that has developed an open source library for loading pretrained transformer models. They also distribute many models that have been pretrained using language modelling tasks, or fine-tuned to specific downstream NLP tasks.  It is currently the best library to use to create NLP models on top of large, deep neural networks. This is especially useful for tasks where simpler, feature-based methods or smaller LSTM models do not perform well enough, for example, when complex processing of syntax and semantics is required (natural language 'understanding'). \n",
    "\n",
    "Let's start by looking at two key types of object in the transformers library: models and tokenizers.\n",
    "\n",
    "## 4.1. Models\n",
    "\n",
    "The neural network models available in the Transformers library are accessed through wrapper classes such as `AutoModel`. If we want to load a pretrained model, we can simply pass its name to the `from_pretrained` function, and the pretrained model weights will be downloaded from HuggingFace and a neural network model will be created with those weights. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122d993d-e48c-4dc2-bc14-2949b3d67e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.bias', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.0.weight', 'fit_denses.3.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'fit_denses.1.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.4.bias', 'cls.seq_relationship.bias', 'fit_denses.4.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel # For BERTs\n",
    "\n",
    "model = AutoModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe226ec-580f-4022-97b0-e5010ddfb55e",
   "metadata": {},
   "source": [
    "This code loads the TinyBERT model, which is a compressed version of BERT. It has 4.4 million parameters, compared to the standard version of BERT, 'BERT-base', which has 110 million parameters. While TinyBERT will not perform as well as larger models, we will use it for this notebook to save memory and computation costs. See [documentation here](https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D).\n",
    "\n",
    "<!--the RoBERTa variant of BERT. It has 4.4 million parameters, compared to the standard version of BERT, 'BERT-base', which has 110 million parameters. While RoBERTa-tiny will not perform as well as larger models, we will use it for this notebook to save memory and computation costs. See [documentation here](https://huggingface.co/arampacha/roberta-tiny).  -->\n",
    "\n",
    "The same functions can be used to load other models from HuggingFace's repository simply by changing the model's name. Take a look at [the Models page](https://huggingface.co/models) so see what there is on offer. Do you recognise any of the models' names?\n",
    "\n",
    "# 4.2. Tokenizers\n",
    "\n",
    "Before we can apply a model to some text, we need to a create Tokenizer object. In Transfomers, Tokenizer objects convert raw text to a sequence of numbers. First, the tokenizer actually performs tokenization, then it maps each token to its numerical ID. There are lots of different tokenizers that we can use to preprocess text. If we are loading a pretrained model, we will need to choose the tokenizer that corresponds to that model. \n",
    "\n",
    "We can load the right tokenizer as follows, in the same way we loaded the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24395ef-aa4a-4d62-a6ea-aa97d86f42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ed12c-cb5a-48c5-b67f-7d17b0a79514",
   "metadata": {},
   "source": [
    "Let's see what the TinyBERT tokenizer does to an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e5852f-46b3-4e28-b952-780f472b6a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'transform', '##er', 'architecture', 'has', 'transformed', 'the', 'field', 'of', 'nl', '##p', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The transformer architecture has transformed the field of NLP.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807c966-188f-4a0b-b435-4282d5aa0201",
   "metadata": {},
   "source": [
    "Let's compare with the NLTK tokenizer we have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee63d96e-2441-4591-bff9-3a1bf6289828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'transformer', 'architecture', 'has', 'transformed', 'the', 'field', 'of', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(sentence)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f2123-34a6-4095-9981-2966fdadd7d8",
   "metadata": {},
   "source": [
    "While NLTK keeps whole words as tokens, the BERT tokenizer splits some words into sub-words and inserts some special characters into the tokens. Splitting is applied to words with low frequency in the training set, such as 'transformer'. \n",
    "\n",
    "**TO-DO 4a:** What is the benefit of splitting rare words into sub-word tokens? **(2 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE:\n",
    "\n",
    "The benefit of this is is that it can deal with words that havent been seen before during training as they are treated as out-of-vocabulary. It also can drastically reduce the size of the vocabulary that the model needs to handle, making the model more memory efficient and speed up training and inference. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "It is important to use the right tokenizer with a pretrained model as each model was trained with text tokenized in a particular way. After tokenization, the Tokenizer object can also map the tokens to their IDs (indexes in the vocabulary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950440ae-d58f-4675-8fb4-6114462331fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1996, 10938, 2121, 4294, 2038, 8590, 1996, 2492, 1997, 17953, 2361, 1012]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17f77c-46f4-4ab0-9eee-858fd0b237f2",
   "metadata": {},
   "source": [
    "## 4.3. Contextualised Embeddings\n",
    "\n",
    "Now that we have a sequence of tokens, we are almost ready to process the sequence using the pretrained model. \n",
    "\n",
    "Our model takes as input a PyTorch `tensor` object. In PyTorch, `tensor` is a muli-dimensional matrix. Here, we need a two-dimensional matrix, where each row is a sequence of input tokens corresponding to a single sentence or document. Let's convert our list of IDs to a 2-D tensor with a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07007371-2125-4fb0-8511-63ade9a2f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1996, 10938,  2121,  4294,  2038,  8590,  1996,  2492,  1997, 17953,\n",
      "          2361,  1012]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ids_tensor = torch.tensor([ids])\n",
    "\n",
    "print(ids_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508047c1-96fa-4839-9eb3-8e64e91c5919",
   "metadata": {},
   "source": [
    "Now we can process the sequence using our model. The model maps the sequence of input IDs to a sequence of output vectors, which are contextualised word embeddings. The hidden state values produced in the last hidden layer of the model are used as the contextualised embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494c8335-7c8a-47c2-b450-a3b775f2262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complete model outputs: \n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3608,  0.2862, -0.1549,  ..., -0.2064,  0.2663, -0.0109],\n",
      "         [ 0.0149,  0.7223, -0.0508,  ..., -0.5505,  0.2355, -0.2962],\n",
      "         [ 0.1531,  0.5903, -0.1244,  ..., -0.4263,  0.0417, -0.1839],\n",
      "         ...,\n",
      "         [ 0.1742, -0.1091, -0.1963,  ..., -0.6736,  0.0472, -0.1840],\n",
      "         [ 0.2434,  0.1021, -0.2241,  ..., -0.5400, -0.1691, -0.1314],\n",
      "         [ 0.0854,  0.3272, -0.3016,  ..., -0.2154, -0.5632, -0.1921]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.1380e-02, -6.3006e-03,  1.8521e-02,  7.1139e-03, -3.1795e-02,\n",
      "          1.3882e-02, -1.5459e-02, -1.0610e-03, -1.8263e-02, -3.6515e-02,\n",
      "         -2.1257e-02, -1.5479e-02, -2.8092e-04, -4.1093e-02, -2.5315e-02,\n",
      "         -4.3338e-02, -1.1616e-03, -1.3931e-02,  6.0733e-03,  4.3790e-03,\n",
      "          2.7090e-04, -2.1810e-02, -4.8026e-02,  2.5493e-02, -1.6502e-02,\n",
      "         -1.2034e-03,  4.2757e-02,  3.0715e-03, -2.3439e-02, -2.5849e-02,\n",
      "         -1.1970e-02,  3.4340e-02, -1.9022e-04, -2.9000e-03, -4.1984e-02,\n",
      "          2.2932e-02,  4.0502e-02,  2.0257e-02, -3.0206e-03, -1.5759e-02,\n",
      "         -4.6124e-02,  6.4128e-03, -1.3939e-02,  4.7155e-02, -2.6471e-03,\n",
      "          3.4991e-03, -4.6633e-03,  8.5901e-03, -5.5664e-02,  9.6636e-03,\n",
      "         -1.9049e-02, -2.3983e-02,  6.6501e-03, -8.3702e-03, -1.4176e-02,\n",
      "          5.6313e-03,  4.2980e-03,  2.4337e-02, -5.4480e-04,  1.1454e-02,\n",
      "         -1.0526e-02, -2.5978e-02, -2.8789e-03, -1.9641e-02, -7.0221e-03,\n",
      "          1.9534e-02,  1.2682e-02, -5.3417e-02, -2.8265e-03,  7.8938e-03,\n",
      "         -4.0802e-02,  3.5494e-02, -8.6865e-03, -1.3383e-02,  1.6434e-03,\n",
      "          5.4638e-03,  1.8293e-02, -1.7385e-02,  3.4079e-02, -3.3782e-02,\n",
      "         -1.4324e-02,  3.0941e-02, -2.2349e-02, -1.4385e-02,  1.6196e-03,\n",
      "          2.3094e-02, -2.1322e-02,  2.5966e-02, -1.5705e-02, -2.1027e-03,\n",
      "         -1.5518e-02,  2.8620e-02,  1.5401e-02, -3.7512e-02, -6.7670e-03,\n",
      "         -8.8314e-03, -1.0061e-02, -1.0518e-02, -1.4193e-02,  4.8184e-02,\n",
      "         -4.2733e-02, -6.8391e-04,  8.1719e-03,  6.9233e-03, -2.9024e-04,\n",
      "         -1.6003e-02, -5.5384e-03,  8.2021e-03,  1.7942e-02, -2.9106e-02,\n",
      "          3.7280e-02,  2.1299e-04,  4.1467e-02, -3.1194e-02, -2.8118e-02,\n",
      "         -1.1518e-02,  4.7534e-02, -2.4884e-02, -2.2532e-02,  1.1021e-02,\n",
      "          4.2616e-02, -2.3387e-02,  3.1372e-02,  2.8327e-03,  9.1081e-03,\n",
      "          2.9967e-02,  1.8475e-02, -6.1313e-02,  1.1888e-02, -1.0234e-02,\n",
      "         -1.7436e-02, -2.5382e-03, -2.0706e-02,  6.2740e-03,  3.5936e-02,\n",
      "         -2.7933e-02,  2.7344e-02,  5.7079e-02, -1.6678e-02,  2.4630e-02,\n",
      "          2.6223e-02,  6.1134e-03,  1.2738e-02, -3.5122e-02, -6.0352e-02,\n",
      "          3.7475e-02, -2.0558e-02, -1.1147e-02, -2.5519e-02, -1.6405e-02,\n",
      "         -5.6021e-03,  3.6427e-02, -3.9644e-03,  6.1234e-03,  1.0637e-02,\n",
      "          2.0472e-02,  3.1075e-03,  1.8298e-02, -7.4329e-03,  1.4522e-02,\n",
      "         -3.2961e-02,  4.0197e-02, -4.9703e-02,  1.5634e-02,  3.3699e-02,\n",
      "          4.1567e-02,  3.4694e-02, -3.8650e-02,  4.7024e-02,  4.5112e-03,\n",
      "         -3.7264e-02, -3.8624e-02, -1.5380e-02,  9.4787e-03, -5.9183e-04,\n",
      "          2.2612e-02,  1.6623e-02,  3.8173e-02,  1.8444e-02, -2.8130e-02,\n",
      "         -8.9479e-04,  9.6280e-03,  1.0077e-02, -3.0724e-02,  2.3116e-02,\n",
      "          2.2716e-02, -2.7906e-02, -2.4386e-02,  6.4444e-03,  4.0045e-02,\n",
      "         -3.8941e-02, -1.3221e-03,  3.2294e-02,  1.9687e-02,  1.5107e-03,\n",
      "          2.2807e-03,  1.1437e-02,  2.1764e-03,  2.4419e-02,  2.5356e-02,\n",
      "         -3.9937e-02,  3.6392e-02, -1.4529e-03,  4.5135e-03, -3.1404e-02,\n",
      "         -2.7645e-02,  3.7107e-02, -2.8359e-02,  2.1804e-02, -2.8085e-02,\n",
      "          2.3142e-02, -3.7059e-02, -3.2632e-02, -3.4501e-02, -5.5987e-03,\n",
      "          4.1899e-03,  6.4634e-04, -4.4314e-02, -4.9819e-02,  2.1470e-02,\n",
      "          1.3016e-02,  1.4301e-02,  1.8736e-02, -5.1866e-03,  1.5026e-02,\n",
      "         -2.1869e-05,  3.7323e-02, -4.4686e-02,  2.5535e-02, -1.6726e-02,\n",
      "          7.3110e-03,  4.8101e-05, -4.0819e-03, -1.7965e-03,  2.8757e-02,\n",
      "          5.1982e-02, -3.4868e-02,  2.4777e-02, -6.2329e-03, -1.9709e-02,\n",
      "         -2.1922e-02,  3.1252e-02,  1.3890e-02, -1.7581e-02,  3.5594e-02,\n",
      "          4.8341e-03,  2.7993e-03, -2.6515e-02, -1.4557e-02, -3.1771e-02,\n",
      "          4.0668e-02,  3.3960e-02,  1.6408e-02,  3.1032e-03, -1.5013e-02,\n",
      "         -9.6832e-03, -1.3780e-02, -3.2977e-02,  1.1426e-02, -4.0551e-03,\n",
      "         -2.4082e-03,  3.5824e-02, -4.9809e-02,  3.7798e-02,  1.7321e-02,\n",
      "          1.0082e-02,  5.8975e-02, -4.2596e-02,  3.6302e-02, -1.2863e-02,\n",
      "         -3.1070e-02,  1.3231e-03, -2.5990e-02, -1.7098e-02, -1.9114e-02,\n",
      "          6.7507e-03,  1.4090e-02,  2.7720e-02,  2.3714e-02, -1.8923e-02,\n",
      "         -2.0685e-02, -4.4664e-02, -1.6670e-02, -1.3620e-02,  1.5473e-02,\n",
      "         -4.4576e-03,  7.4260e-04, -3.9407e-03,  1.4169e-02, -2.6952e-03,\n",
      "         -2.6419e-02,  2.4067e-02,  4.7123e-02,  8.8112e-03,  1.7120e-02,\n",
      "          1.0671e-02, -2.4989e-03,  4.1720e-03, -3.7655e-02,  1.7643e-02,\n",
      "          1.0087e-02, -9.5763e-03, -2.1317e-02, -4.8866e-02, -1.9793e-02,\n",
      "          2.8578e-02,  1.2889e-02, -1.9376e-02,  4.6893e-02, -8.4308e-04,\n",
      "          1.6833e-03,  4.6231e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "The last hidden state for the first token in the sequence (the first word embedding): \n",
      "tensor([[ 0.3608,  0.2862, -0.1549,  ..., -0.2064,  0.2663, -0.0109],\n",
      "        [ 0.0149,  0.7223, -0.0508,  ..., -0.5505,  0.2355, -0.2962],\n",
      "        [ 0.1531,  0.5903, -0.1244,  ..., -0.4263,  0.0417, -0.1839],\n",
      "        ...,\n",
      "        [ 0.1742, -0.1091, -0.1963,  ..., -0.6736,  0.0472, -0.1840],\n",
      "        [ 0.2434,  0.1021, -0.2241,  ..., -0.5400, -0.1691, -0.1314],\n",
      "        [ 0.0854,  0.3272, -0.3016,  ..., -0.2154, -0.5632, -0.1921]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_outputs = model(ids_tensor)\n",
    "print('The complete model outputs: ')\n",
    "print(model_outputs)\n",
    "\n",
    "print()\n",
    "print('The last hidden state for the first token in the sequence (the first word embedding): ')\n",
    "embeddings = model_outputs['last_hidden_state'][0]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee4181-7518-484f-993d-f7abffcae010",
   "metadata": {},
   "source": [
    "We can retrieve the embedding vector for \"transform\" like this (\"transform\" is the second token in the sequence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3df76db-f81e-4916-b31d-34a8341a20db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.49155427e-02  7.22318113e-01 -5.07865176e-02 -2.74206609e-01\n",
      " -1.38931692e-01  1.00099540e+00  7.11429864e-03  2.71392524e-01\n",
      " -3.92823443e-02  6.04107268e-02  1.25740260e-01  4.60632175e-01\n",
      "  6.25270605e-03  1.61929548e-01  1.23913646e-01 -4.08096492e-01\n",
      "  1.24867544e-01 -4.71536160e-01  2.24768862e-01  6.35189414e-02\n",
      "  8.56178999e-02 -1.88045263e-01  1.77257776e-01  3.40049475e-01\n",
      " -1.95545897e-01  1.58554405e-01  9.62863415e-02  1.12649783e-01\n",
      "  2.21045017e-01 -9.56112981e-01 -3.85949075e-01  1.39220759e-01\n",
      "  5.90011537e-01 -8.06727767e-01 -1.34287819e-01  2.35692143e-01\n",
      " -1.02274813e-01  2.78302789e-01  7.94321358e-01 -2.49362707e-01\n",
      "  1.72772393e-01 -2.07582787e-01  3.00156832e-01 -8.59342963e-02\n",
      " -2.25284770e-01 -9.75407809e-02 -3.52349520e-01  3.81161153e-01\n",
      " -3.87681544e-01 -1.77613750e-01 -4.13686156e-01  1.38046280e-01\n",
      "  1.29867718e-02  6.52684391e-01  1.16502836e-01 -5.10778427e-01\n",
      " -8.30404162e-02 -2.67044231e-02  3.12863350e-01 -2.62848616e-01\n",
      " -1.43285260e-01  1.10270239e-01 -1.46889180e-01  8.40611607e-02\n",
      " -5.41599095e-01 -2.74970289e-02 -2.71180451e-01  3.61281693e-01\n",
      "  1.86116874e-01 -2.27868736e-01 -8.60139951e-02  4.15635169e-01\n",
      " -2.35570576e-02  2.90851772e-01 -4.29375321e-01  5.02850115e-01\n",
      "  4.46188778e-01 -3.45922112e-01 -1.07706375e-01  9.98821408e-02\n",
      "  4.75118756e-01  1.42710775e-01  6.47548959e-03  1.91746801e-02\n",
      "  4.32648361e-01 -7.28580281e-02  1.23887286e-01  4.31222141e-01\n",
      " -2.87044287e-01  8.61874580e-01 -3.77015769e-01 -2.96231687e-01\n",
      " -6.53620064e-02 -1.08727634e-01 -5.34366786e-01  2.30699390e-01\n",
      " -4.43290472e-02  8.28530546e-03 -4.80109811e-01  6.32632002e-02\n",
      "  3.46862793e-01 -1.00397058e-02  2.46484399e-01  1.04727946e-01\n",
      "  5.85353971e-02 -1.73106864e-01 -1.63396329e-01 -1.42522052e-01\n",
      " -1.41767800e-01  5.59852384e-02  2.56438792e-01 -3.92891139e-01\n",
      " -3.47413808e-01 -5.97775280e-01 -1.29221320e-01 -3.44479710e-01\n",
      "  7.78069422e-02  2.47690767e-01  3.36303830e-01 -1.69770136e-01\n",
      "  5.26939854e-02 -4.30812657e-01 -7.06476927e-01  2.43629497e-02\n",
      " -2.37541378e-01 -3.28976274e-01  1.46830723e-01  4.63140100e-01\n",
      " -7.27379546e-02  4.38388288e-02  4.04198430e-02 -3.49178351e-02\n",
      " -5.20068556e-02 -1.70057774e-01  3.05907484e-02  9.56190601e-02\n",
      "  1.73355973e+00  2.14067742e-01  6.37326688e-02  5.80559313e-01\n",
      " -9.82592255e-02 -4.88570511e-01  2.82070935e-01  2.64479309e-01\n",
      "  3.23805898e-01 -2.94937968e-01  1.32838845e-01 -3.09568793e-02\n",
      "  1.98950231e-01  1.01507120e-01  3.46358567e-01  4.12193447e-01\n",
      " -2.06010640e-01  1.38656542e-01  8.95754695e-02 -9.03490037e-02\n",
      "  1.60650350e-02  1.04298651e-01  5.76754689e-01  4.83327985e-01\n",
      "  1.27746224e-01 -4.36358526e-02  2.58013576e-01 -7.33276457e-02\n",
      " -3.81114244e-01 -4.86686006e-02  5.54442465e-01 -2.71809399e-02\n",
      " -3.28380078e-01  1.03765115e-01 -7.67536685e-02 -2.55299687e-01\n",
      " -4.58066493e-01 -6.74323887e-02 -5.92074543e-02 -5.33038415e-02\n",
      "  7.31044054e-01 -4.37687375e-02  1.48426831e-01 -6.68217838e-01\n",
      "  5.39265037e-01 -5.51167607e-01  3.07089388e-01 -2.95280635e-01\n",
      "  2.30028942e-01  4.45130497e-01  5.98997623e-02  3.32111120e-01\n",
      "  3.09372127e-01  7.49412835e-01  6.80035591e-01  2.81824023e-01\n",
      " -1.52892470e-01 -2.78219014e-01  1.57606661e-01 -3.44320610e-02\n",
      " -4.25325543e-01  3.65774840e-01 -6.05315976e-02 -2.44895816e-01\n",
      " -9.54144746e-02  2.14865655e-01  3.48819941e-02  4.22626734e-01\n",
      "  1.17061369e-01  1.53851435e-01  4.89412874e-01 -4.79475707e-02\n",
      " -4.35449898e-01  1.52308494e-01 -3.75605643e-01 -2.00069591e-01\n",
      " -4.52390373e-01 -9.23971385e-02  7.94963017e-02 -3.91879797e-01\n",
      "  2.03301355e-01  5.86735785e-01  1.18399151e-02 -3.49965125e-01\n",
      " -1.03549659e-02  1.67385578e-01  1.03552282e-01  1.20481970e-02\n",
      "  2.16727853e-01 -6.50196746e-02 -3.98291290e-01 -2.12185472e-01\n",
      "  1.36516765e-02 -5.55399001e-01 -2.92274415e-01  1.41717881e-01\n",
      "  5.35454035e-01  2.17892587e-01 -4.19899046e-01 -3.32855552e-01\n",
      "  3.76726270e-01 -3.19371164e-01 -3.27617556e-01  4.93491292e-01\n",
      "  1.59247264e-01  7.08295047e-01 -1.90098122e-01  5.06709993e-01\n",
      " -6.10249400e-01 -6.68389946e-02  1.14125752e+00  8.79686549e-02\n",
      " -1.85308099e-01 -1.45169184e-01  2.87572980e-01 -4.92560685e-01\n",
      "  3.61903943e-02  2.62837678e-01 -2.52179325e-01  6.15258217e-02\n",
      " -3.54109854e-01 -3.53946030e-01  1.98717549e-01  1.07959282e+00\n",
      "  1.29683304e+00  3.58777165e-01  6.75653517e-01  2.51170009e-01\n",
      " -5.65679789e-01 -4.81561691e-01  3.10951740e-01 -1.07663371e-01\n",
      " -3.67720991e-01  8.12476724e-02 -4.20884013e-01  5.78671575e-01\n",
      "  9.49739739e-02  1.65470123e-01  3.70412618e-01 -7.44711189e-03\n",
      "  4.63328183e-01 -2.38428533e-01  4.81733307e-02 -1.74412474e-01\n",
      "  8.61790329e-02 -1.35659158e-01  2.68296123e-01  2.98456430e-01\n",
      "  6.36236191e-01  2.79195786e-01 -1.57060534e-01 -2.91061014e-01\n",
      " -1.38483956e-01 -9.19548497e-02 -3.60317588e-01 -3.03814471e-01\n",
      "  2.38154992e-01  3.61208528e-01  1.53050736e-01  5.15788980e-02\n",
      " -1.36773288e-03  9.56998616e-02 -3.30190152e-01  3.86729956e-01\n",
      " -2.75555283e-01  3.30927014e-01 -2.23884821e-01 -4.71363157e-01\n",
      "  8.31369832e-02 -2.26589739e-01  2.15097278e-01  2.63798445e-01\n",
      "  3.72823089e-01 -5.50536096e-01  2.35453397e-01 -2.96165347e-01]\n",
      "The TinyBERT embeddings have 312 dimensions.\n"
     ]
    }
   ],
   "source": [
    "emb = embeddings[1]  # get second embedding in the sequence\n",
    "\n",
    "# convert it to a numpy array so we can perform various operations on it later on\n",
    "emb = emb.detach().numpy()\n",
    "\n",
    "print(emb)\n",
    "print(f'The TinyBERT embeddings have {emb.shape[0]} dimensions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1d779-8ec7-462a-895b-e58a88d9bb63",
   "metadata": {},
   "source": [
    "TO-DO 4b: Retrieve the embedding for \"architecture\" (this to-do will not be marked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "827ca93a-d80d-4655-a417-6c6c6d1e07f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71387130e-01  7.74581850e-01 -3.24257016e-01 -7.14323670e-02\n",
      " -4.95385379e-04  9.37310040e-01 -4.40284982e-03 -4.26921584e-02\n",
      "  1.27404267e-02  1.89277846e-02  1.02527849e-01  4.54655915e-01\n",
      "  2.70436138e-01  2.30988413e-01  4.03663330e-03 -1.08995482e-01\n",
      " -4.59910110e-02 -3.51154387e-01 -1.34710342e-01  8.29388499e-02\n",
      "  1.86496913e-01  5.00274561e-02  7.21661821e-02  2.28657156e-01\n",
      " -2.19696999e-01  9.40199867e-02  1.65539995e-01  1.85794756e-01\n",
      "  3.17783266e-01 -5.09367108e-01 -5.00949562e-01  1.52487800e-01\n",
      "  4.57999200e-01 -8.51875782e-01 -1.58632979e-01  1.58965096e-01\n",
      "  4.16193828e-02  2.30997890e-01  8.78503144e-01 -6.23159632e-02\n",
      "  1.87219620e-01 -1.23370588e-02  2.10084260e-01  3.48071381e-02\n",
      " -2.51240134e-01 -1.37914822e-01 -3.88696730e-01  2.98189640e-01\n",
      " -2.92033404e-01 -3.19503993e-01 -1.98435634e-01  1.32032171e-01\n",
      " -6.46373034e-02  7.43182778e-01  7.14239180e-02 -3.02118361e-01\n",
      "  3.49781871e-01 -5.81784546e-02  2.85069406e-01 -4.09580857e-01\n",
      " -1.03297204e-01  1.03767656e-01 -2.22905397e-01  8.86315703e-02\n",
      " -4.33602333e-01  2.17865974e-01 -2.76991636e-01  3.99589002e-01\n",
      "  1.69203937e-01 -3.05192411e-01 -2.66391337e-01  7.28380799e-01\n",
      " -2.11687595e-01  3.97848904e-01 -5.89087367e-01  4.29774702e-01\n",
      "  4.25439090e-01 -2.14259833e-01 -9.17085037e-02  2.12369531e-01\n",
      "  4.30577934e-01 -1.41512185e-01  2.11011358e-02  1.92521542e-01\n",
      "  3.87896419e-01 -2.28493258e-01  2.32072353e-01  4.97907937e-01\n",
      " -1.22621924e-01  5.94609261e-01 -3.73322636e-01 -3.34645361e-01\n",
      " -1.71525300e-01 -1.55617654e-01 -6.56991184e-01  1.63026884e-01\n",
      " -1.98499672e-02 -1.46204665e-01 -4.47816700e-01  1.86819658e-01\n",
      "  2.59208620e-01  8.59290212e-02  3.39515775e-01 -1.29848391e-01\n",
      " -1.14089012e-01 -2.00275704e-01 -2.15664297e-01 -5.63766249e-02\n",
      " -1.07931159e-01 -4.68209833e-02  2.56857395e-01 -2.77532607e-01\n",
      " -2.11152453e-02 -5.35417318e-01 -7.24147707e-02 -4.13410276e-01\n",
      "  1.73524633e-01  1.13118373e-01  2.38801524e-01 -2.24455044e-01\n",
      "  8.62925202e-02  1.15332156e-02 -3.62385333e-01  2.29010627e-01\n",
      " -3.50627303e-01 -3.97162884e-01 -5.64109161e-03  5.43279469e-01\n",
      "  4.42473143e-02  2.74848461e-01 -4.32900228e-02 -8.17514062e-02\n",
      "  2.26518288e-01 -2.17810094e-01 -2.09805012e-01  3.68050605e-01\n",
      "  1.74909031e+00  2.32437804e-01  4.12015319e-02  4.28760469e-01\n",
      " -7.73503557e-02 -3.94461036e-01  4.56214964e-01  3.94748747e-01\n",
      "  3.47979933e-01 -1.98664173e-01  1.66281071e-02  4.17210422e-02\n",
      "  2.80723989e-01  1.72675833e-01  3.71788591e-01  4.58930522e-01\n",
      " -2.03692019e-01  2.08810598e-01  1.32481873e-01  4.34805453e-03\n",
      " -1.48096830e-01  1.11546852e-01  5.05591333e-01  6.24570310e-01\n",
      "  5.56748882e-02 -1.93018794e-01  1.44752994e-01 -2.42132172e-01\n",
      " -3.77227992e-01 -8.83389041e-02  3.18628818e-01  1.29283965e-03\n",
      " -4.68146443e-01  1.09013915e-01 -9.88199785e-02  4.33953330e-02\n",
      " -5.95691621e-01 -6.29965216e-02  1.90070122e-01 -6.22256473e-02\n",
      "  4.44375247e-01 -4.45064344e-02  3.63634378e-02 -7.49727547e-01\n",
      "  6.06420696e-01 -5.94443738e-01  2.27024272e-01 -3.24530840e-01\n",
      "  2.27765322e-01  2.53754437e-01  3.37927341e-02  3.54703546e-01\n",
      "  1.48369774e-01  8.01495075e-01  7.37674892e-01  3.16927552e-01\n",
      " -4.65496212e-01 -2.19995663e-01  8.55115652e-02  7.90397897e-02\n",
      " -4.21971291e-01  3.00321996e-01 -1.64274462e-02 -3.05002004e-01\n",
      " -1.29668638e-01  5.97734116e-02  2.27804169e-01  2.51437157e-01\n",
      "  1.32936984e-01  1.77337095e-01  5.35019577e-01 -1.26854032e-01\n",
      " -3.70967776e-01  1.97892040e-01 -5.81951022e-01 -8.73126760e-02\n",
      " -4.38537776e-01  4.81506810e-02  4.27304916e-02 -3.52274030e-01\n",
      "  9.06556398e-02  8.38536978e-01  1.90427706e-01 -3.27884287e-01\n",
      "  5.39015420e-02  2.09419802e-01  2.61073232e-01  1.41682014e-01\n",
      "  2.57945895e-01  1.15054712e-01 -2.29461014e-01 -3.17161620e-01\n",
      "  2.03591198e-01 -4.94222999e-01 -1.88396364e-01  3.99250805e-01\n",
      "  3.61400992e-01  1.02145784e-01 -6.39539301e-01 -4.55038756e-01\n",
      "  3.48606646e-01 -3.01878452e-01 -3.11838746e-01  5.37506878e-01\n",
      "  1.79695249e-01  9.53896880e-01 -9.25653577e-02  2.97179103e-01\n",
      " -4.64271128e-01 -1.54976875e-01  1.04017854e+00 -5.71001805e-02\n",
      " -1.25494301e-01 -1.59981072e-01  3.02586377e-01 -6.54342115e-01\n",
      "  8.32995474e-02  2.96592772e-01 -2.88193554e-01 -5.19622564e-02\n",
      " -1.16057947e-01 -4.02866811e-01 -1.85678601e-02  8.54045451e-01\n",
      "  1.04417574e+00  2.02207699e-01  3.87416303e-01  2.76725143e-01\n",
      " -6.04186416e-01 -4.56754088e-01  1.98381543e-01 -1.97067857e-01\n",
      " -2.95010448e-01  2.78941929e-01 -6.31202459e-01  5.56932867e-01\n",
      "  2.03483492e-01  9.17818397e-02  2.54568428e-01 -3.95415761e-02\n",
      "  2.62132764e-01 -2.75719345e-01  3.52464676e-01 -3.85204822e-01\n",
      "  2.10844651e-01 -8.33531469e-03  2.20536232e-01 -2.24064328e-02\n",
      "  5.56668162e-01  1.01012379e-01 -7.73577243e-02 -4.63616133e-01\n",
      " -3.20619702e-01 -5.78197986e-02 -2.29295582e-01 -2.77262032e-01\n",
      "  1.52324289e-01  4.02601004e-01  1.10905178e-01  1.83826119e-01\n",
      " -3.95610258e-02  1.69774015e-02 -5.05567372e-01  6.35553151e-02\n",
      "  6.98359907e-02  5.73959351e-01 -1.97177440e-01 -3.72904509e-01\n",
      "  1.01819962e-01 -1.72659144e-01  2.11300582e-01  1.70526430e-01\n",
      "  2.35796541e-01 -4.73861575e-01 -1.39762938e-01 -2.52476901e-01]\n",
      "The TinyBERT embeddings for \"architecture\" have 312 dimensions.\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "\n",
    "position = tokens.index('architecture')\n",
    "\n",
    "# Get the embedding for 'architecture'\n",
    "architecture_emb = embeddings[position]\n",
    "\n",
    "# Convert it to a numpy array\n",
    "architecture_emb = architecture_emb.detach().numpy()\n",
    "\n",
    "print(architecture_emb)\n",
    "print(f'The TinyBERT embeddings for \"architecture\" have {architecture_emb.shape[0]} dimensions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c79402-f6b7-4fc3-9f47-50fcffccbe68",
   "metadata": {},
   "source": [
    "Sentences and documents usually have varying lengths. So, to put multiple sentences into a single tensor, we need to pad the sequences up to a maximum length. Luckily, the tokenizer class takes care of this for us. When we pass in a list of sentences, the tokenizer creates a matrix, where each row is a sequence of the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03abaed-6738-44ab-a7dd-7775743f99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2027,  2363,  1037,  5414,  2013,  1996,  2924,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2009,  2001,  2025,  2204,  2005,  2593,  2010,  2924,  5703,\n",
      "          2030,  2010,  2668,  3778,  1012,   102],\n",
      "        [  101,  2016,  2939,  2247,  1996,  2924,  1997,  1996,  2314,  2875,\n",
      "          1996,  2103,  1012,   102,     0,     0],\n",
      "        [  101,  2027,  2924,  2037, 18178, 10997,  2006,  9432,  2015,  1012,\n",
      "           102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2016,  2939,  2247,  1996, 22756,  2875,  1996,  2103,  1012,\n",
      "           102,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"They received a loan from the bank.\",\n",
    "    \"It was not good for either his bank balance or his blood pressure.\",\n",
    "    \"She walked along the bank of the river towards the city.\",\n",
    "    \"They bank their cheques on Thursdays.\",\n",
    "    \"She walked along the embankment towards the city.\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")  \n",
    "\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "387dc219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Confirming the value of the padding token \n",
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a44fc-218e-462d-86ef-fee203417bb6",
   "metadata": {},
   "source": [
    "`model_inputs` is a dictionary containing three objects:\n",
    " * The `input_ids` are the list of token IDs in the input sequences. \n",
    " * The `attention_mask` records which tokens are special padding tokens and which are real tokens. Tokens with a 0 in the attention mask will be ignored.\n",
    " * `token_type_ids` is needed when two sequences are passed together as input to the model for tasks such as next sentence prediction that involve comparing two sentences. Here, each input is a single sentence, so we have only one type of token in the output above. \n",
    " \n",
    "TO-DO 4c: What value do the special padding tokens have? (this to-do is unmarked)\n",
    "\n",
    "ANSWER: \n",
    "\n",
    "0\n",
    "\n",
    "---\n",
    "\n",
    "Notice that the input_ids all start with the same token ID, 101, even though they have different first words. They also have token ID 102 before the padding tokens. This is because the tokenizer inserts two special tokens, which are used in some applicaions of BERT. 101 is the '[CLS]' token, which is a dummy token whose embedding can be trained to represent the whole sequence. the [CLS] token's embedding can then be used as input to a text classifier to classify a sentence or document. Token 102 is '[SEP]', which can be used to separate multiple input sequences in a single example. This is needed in tasks where multiple pieces of text are provided as input, e.g., a to build a classifier that can determine whether two sentences contradict each other. \n",
    "\n",
    "We can now pass all of the model inputs to the model to produce a set of contextualised embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b888b7cf-54d1-450b-9431-e64629177d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs is a dictionary, so to provide the arguments to model(), \n",
    "# we use the double star to unpack the dictionary so that each key in the dictionary is\n",
    "# an argument to model() and each value is the value of the argument. \n",
    "model_outputs = model(**model_inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8eb74-ce63-459c-9fb3-05fd2768ddf8",
   "metadata": {},
   "source": [
    "**TO-DO 4d:** The first four example sentences above all contain the word \"bank\", and the last example contains \"embankment\". Obtain a list of contextualised word embeddings for 'bank' and 'embankment' in the example sentences using our model. **(4 marks)**\n",
    "\n",
    "Hint: you may need to convert tensors to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0297da-4106-41ef-8f61-5aa3fbb2cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "#WRITE YOUR OWN CODE HERE\n",
    "\n",
    "# Define the words of interest\n",
    "words_of_interest = ['bank', 'embankment']\n",
    "\n",
    "# Tokenize the words of interest\n",
    "tokenized_words = [tokenizer.tokenize(word) for word in words_of_interest]\n",
    "\n",
    "# Initialize an empty list to store the embeddings\n",
    "embeddings_list = []\n",
    "\n",
    "# Iterate over each tokenized sentence\n",
    "for i, input_ids in enumerate(model_inputs['input_ids']):\n",
    "    # Convert input_ids tensor to a list\n",
    "    input_ids_list = input_ids.tolist()\n",
    "    # Decode the input_ids to get the tokenized sentence\n",
    "    tokenized_sentence = tokenizer.convert_ids_to_tokens(input_ids_list)\n",
    "    # Initialize an empty list to store the embeddings for this sentence\n",
    "    embeddings_sentence = []\n",
    "    # Iterate over each tokenized word of interest\n",
    "    for tokenized_word in tokenized_words:\n",
    "        # If the tokenized word is a subword, take only the first subword\n",
    "        if tokenized_word[0] in tokenized_sentence:\n",
    "            # Get the position of the word in the tokenized sentence\n",
    "            position = tokenized_sentence.index(tokenized_word[0])\n",
    "            # Get the embedding for the word\n",
    "            word_embedding = model_outputs[0][i][position]\n",
    "            # Convert it to a numpy array and append to the list\n",
    "            embeddings_sentence.append(word_embedding.detach().numpy())\n",
    "        else:\n",
    "            embeddings_sentence.append(None)\n",
    "    # Append the embeddings for this sentence to the main list\n",
    "    embeddings_list.append(embeddings_sentence)\n",
    "\n",
    "# Convert the main list to a numpy array\n",
    "embeddings_array = np.array(embeddings_list, dtype=object)\n",
    "\n",
    "# Print the shape of the array\n",
    "print(embeddings_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac47b72-46a3-40ee-b081-bca17da0b49b",
   "metadata": {},
   "source": [
    "**TO-DO 4e:** Compute the similarities between these embeddings in the cell below, and show the results. Which embeddings are most similar to one another and why? **(6 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE:\n",
    "\n",
    "The first and second sentences are most similar to each other with a cosine similarity of approximately 0.633. This means that the context in which 'bank' is used in these two sentences is somewhat similar. The reason why these two embeddings are the most similar to one another is because in both sentences the word 'bank' is used in financial contexts. As for 'embankment' it only appears in one sentences so its similarity to the embeddings for 'bank' cant be provided . However since it is similar to the word 'bank' its embedding would likely be similar in the third sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a9c5c1-b6b2-4df4-8729-0f0615b5e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities between \"bank\" embeddings:\n",
      "[[0.9999998  0.6331346  0.4877863  0.5232957 ]\n",
      " [0.6331346  0.9999998  0.43572852 0.4705649 ]\n",
      " [0.4877863  0.43572852 0.99999994 0.47878546]\n",
      " [0.5232957  0.4705649  0.47878546 1.0000001 ]]\n",
      "Similarities between \"embankment\" embeddings:\n",
      "[[1.0000001]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Separate the embeddings for \"bank\" and \"embankment\"\n",
    "bank_embeddings = embeddings_array[:,0]\n",
    "embankment_embeddings = embeddings_array[:,1]\n",
    "\n",
    "# Remove None values\n",
    "bank_embeddings = [emb for emb in bank_embeddings if emb is not None]\n",
    "embankment_embeddings = [emb for emb in embankment_embeddings if emb is not None]\n",
    "\n",
    "# Compute similarities between all pairs of \"bank\" embeddings\n",
    "bank_similarities = cosine_similarity(bank_embeddings)\n",
    "\n",
    "# Compute similarities between all pairs of \"embankment\" embeddings\n",
    "# In this case, it's just one embedding, so the similarity is 1\n",
    "embankment_similarities = cosine_similarity(embankment_embeddings)\n",
    "\n",
    "# Print the similarities\n",
    "print('Similarities between \"bank\" embeddings:')\n",
    "print(bank_similarities)\n",
    "\n",
    "print('Similarities between \"embankment\" embeddings:')\n",
    "print(embankment_similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d6407-24ea-41f5-a16e-d03fe15aa124",
   "metadata": {},
   "source": [
    "**TO-DO 4f:** Use the [CLS] token's embedding to find the most similar **sentence** to \"She walked along the embankment towards the city.\" from the first four sentences. Print the similarities and the selected sentence. **(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32808677-1efb-49aa-a384-cd8ca1632d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between sentence 5 and sentence 1: 0.909304141998291\n",
      "Similarity between sentence 5 and sentence 2: 0.7936346530914307\n",
      "Similarity between sentence 5 and sentence 3: 0.9947961568832397\n",
      "Similarity between sentence 5 and sentence 4: 0.8988915681838989\n",
      "The most similar sentence to \"She walked along the embankment towards the city.\" is:\n",
      "She walked along the bank of the river towards the city.\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n",
    "\n",
    "# Get the [CLS] embeddings for each sentence\n",
    "cls_embeddings = model_outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# Compute similarities between the [CLS] embedding for the last sentence and for the other sentences\n",
    "similarities = cosine_similarity(cls_embeddings[-1].reshape(1, -1), cls_embeddings[:-1])\n",
    "\n",
    "# Print the similarities\n",
    "for i, similarity in enumerate(similarities[0]):\n",
    "    print(f'Similarity between sentence 5 and sentence {i+1}: {similarity}')\n",
    "\n",
    "# Find the most similar sentence\n",
    "most_similar_index = np.argmax(similarities)\n",
    "most_similar_sentence = sentences[most_similar_index]\n",
    "\n",
    "print('The most similar sentence to \"She walked along the embankment towards the city.\" is:')\n",
    "print(most_similar_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768029d-395e-4eb9-ac1b-7c72e81dcd01",
   "metadata": {},
   "source": [
    "# 5. Question Answering with Pretrained Transformers (max. 11 marks)\n",
    "\n",
    "The previous section showed us how to obtain a sequence of contextualised word embeddings using a pretrained transformer. How are these embeddings used to extract answers from documents to a given question?\n",
    "\n",
    "First, let's load up the [Tweet QA](https://huggingface.co/datasets/tweet_qa) dataset, which we will use to test a pretrained question answering (QA) model. This dataset contains tweets along with questions about the information in the tweets, and a list of correct answers. As we are not going to train our own QA model (it requires a lot of compute time), we will only need the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bdc0e14-0241-44f7-97f1-745a33312700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweet_qa (./data_cache/tweet_qa/default/1.0.0/7d588f7f477946b10f60c035ca55175737315ac446102b015218af38d2638777)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset with 1086 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_qa\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Validation dataset with {len(val_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f2f2c-8f39-49b0-8391-33773171c5a9",
   "metadata": {},
   "source": [
    "Now we are working with complete dataset using the HuggingFace datasets library. In the next cell, we create a tokenizer to tokenize the examples in the dataset. We need to choose the right tokenizer for the QA model we want to use, so let's decide to use `\"distilbert-base-cased-distilled-squad\"` as our pretrained model. This is based on a smaller version of BERT, called Distilbert, which was fine-tuned on the SQUAD question answering dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1993c3b2-3454-4cd2-81ed-020342ca3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\") \n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    # Pass two strings to the tokenizer -- it will concatenate them with a [SEP] special token between them. \n",
    "    model_inputs = tokenizer(dataset['Question'], dataset['Tweet'], padding=\"max_length\", max_length=200, truncation='only_second')\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb870b-1812-413a-be0c-c4868b441fe8",
   "metadata": {},
   "source": [
    "Again, we can use the `map()` method to apply the tokenizer to each example in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "617d4300-3994-44a4-8e14-d1835bfbc2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x189c02e50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee74395d5764909896e812176e9e073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset = val_dataset.map(tokenize_function, batched=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ccaf0-fc74-405b-b72f-f03328d6622a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The type of QA model we are going to work with is _extractive_, meaning that the model will extract the answer from the 'context' (also known as the 'passage' or 'source document'). It does this by identifying the index of the start and end tokens of the answer span within the context, or returning `(0, 0)` (the index 0 for both the start and end token) if the context does not contain an answer to the given question. \n",
    "\n",
    "As explained in the lectures, BERT forms the basis of the QA model, and maps each token to a contextualised embedding. The QA model then maps each token's contextualised embedding to the probability that the token is the start of the answer span, and to the probability that the token is the end of the answer span. The layers that map the embeddings to the start and end probabilities are known as the 'head' of the model. [The original BERT paper](https://arxiv.org/pdf/1810.04805.pdf) depicts the QA model like this (Devlin et al., 2018):\n",
    "\n",
    "<img src=\"bert_qa.png\" alt=\"BERT QA diagram from the slides in week 10 showing the embedding of each token connected to the start and end output layers\" width=\"400px\"/>\n",
    "\n",
    "We can see a similar structure in most neural network models. Our original text classifier from the first notebook used a fully-connected layer to produce a hidden representation of the whole sentence (rather than using BERT to produce a sequence of embeddings). This hidden representation was then fed to an output layer to produce a probability distribution over class labels (rather than the start and end probabilities):\n",
    "\n",
    "<img src=\"neural_text_classifier_smaller.png\" alt=\"Neural text classifier diagram from the slides in lecture 8.1\" width=\"400px\"/>\n",
    "\n",
    "\n",
    "<!--With transformers, \n",
    "we can do something very similar, by connecting the transfomer's output to a fully-connected layer. However, with BERT, we do not need to pass the embedding of each individual word to the fully-connected layer because there is a special [CLS] token that represents the whole sentence:\n",
    "\n",
    "The code below shows how to access a tensor containing the [CLS] embeddings:-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff251c-ccd2-4e0c-ab2b-a1bd1cc38101",
   "metadata": {},
   "source": [
    "Now, we have the dataset in the right format, let's see how to load a pretrained QA model based on a pretrained transformer. The QA model was trained by taking a pretrained BERT model (pretrained on masked language modelling with unlabelled text), adding the QA head, then further training the complete model on a QA dataset. \n",
    "\n",
    "The transformers library provides some useful wrapper classes for loading pretrained models for various NLP tasks, such as QA or text classification. These 'auto' classes are documented here: https://huggingface.co/docs/transformers/model_doc/auto . Let's use an auto class to load the `\"distilbert-base-cased-distilled-squad\"` pretrained QA model (this code will try to reload the model from a cache or download the model from HuggingFace):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46229f2c-86f0-4150-9b1d-72b0df714ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b470e-d580-492f-807e-466cd2193ab3",
   "metadata": {},
   "source": [
    "As our model was pretrained, we can use it directly on our Tweet_QA dataset (you may see a message to this effect when you run the cell above the first time). \n",
    "\n",
    "So, how do we get a prediction from the model? Let's take a single example from Tweet_QA and obtain the start and end probabilities for all tokens in the 'context':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef26c556-7230-40b3-a06a-0e823556415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(qa_model, dataset):\n",
    "    \n",
    "    # Switch off dropout\n",
    "    qa_model.eval()\n",
    "\n",
    "    # Pass the required inputs from the dataset to the model    \n",
    "    output = qa_model(attention_mask=torch.tensor(dataset[\"attention_mask\"]), input_ids=torch.tensor(dataset[\"input_ids\"]))\n",
    "        \n",
    "    # the output dictionary contains logits, which are the unnormalised scores for each class for each example:\n",
    "    probs_start = torch.nn.Softmax(dim=1)(output[\"start_logits\"]).detach().numpy()\n",
    "    probs_end = torch.nn.Softmax(dim=1)(output[\"end_logits\"]).detach().numpy()\n",
    "        \n",
    "    return probs_start, probs_end\n",
    "\n",
    "# Run the prediction function to get the results for the first 20 examples:\n",
    "probs_start, probs_end = predict_nn(model, val_dataset[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad86be8-d404-43ea-a341-8e46603478f8",
   "metadata": {},
   "source": [
    "Now that we have the probabilities that each token is a start or end token, we combine these probabilities to estimate the probability of each possible answer span. This will allow us to choose the answer span with highest probability. \n",
    "\n",
    "In the next cell is our first attempt, which you will need to improve to get valid answers. This code loops through each possible combination of start and end tokens, obtains the start and end probabilities, and extracts the answer text for the corresponding span.\n",
    "\n",
    "**TO-DO 5a:** Use the start and end probabilities to compute the answer span probability at the place marked inside the predict_answer() function below. **2 marks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "660f8ada-810d-4764-b0c2-ac7b291ed9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT = The #endangeredriver would be a sexy bastard in this channel if it had water. Quick turns. Narrow. (I'm losing it) John D. Sutter (@jdsutter) June 21, 2014\n",
      "QUESTION = what hashtag was used?\n",
      "LIST OF POSSIBLE ANSWERS = ['#endangeredriver', '#endangereddriver']\n"
     ]
    }
   ],
   "source": [
    "# our example:\n",
    "example_index = 3\n",
    "\n",
    "example = val_dataset[example_index]\n",
    "print(f'CONTEXT = {example[\"Tweet\"]}')\n",
    "print(f'QUESTION = {example[\"Question\"]}')\n",
    "print(f'LIST OF POSSIBLE ANSWERS = {example[\"Answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22088d8f-ac0a-43b5-9f7e-1a71feec8f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span prob = 0.5619075298309326, answer = endangeredriver\n",
      "Span prob = 0.15243792533874512, answer = The # endangeredriver\n",
      "Span prob = 0.12783198058605194, answer = # endangeredriver\n",
      "Span prob = 0.04335250332951546, answer = \n",
      "Span prob = 0.018347227945923805, answer = endangeredriver would be a sexy bastard in this channel if it had water. Quick turns. Narrow\n",
      "Span prob = 0.01445400808006525, answer = \n",
      "Span prob = 0.0079822838306427, answer = endangeredriver would be a sexy bastard in this channel if it had water. Quick turns\n",
      "Span prob = 0.007700338959693909, answer = endangeredriver would be a sexy bastard in this channel if it had water\n",
      "Span prob = 0.007665039040148258, answer = \n",
      "Span prob = 0.006627385504543781, answer = ##river\n",
      "Span prob = 0.005845780950039625, answer = \n",
      "Span prob = 0.004977355245500803, answer = The # endangeredriver would be a sexy bastard in this channel if it had water. Quick turns. Narrow\n",
      "Span prob = 0.004173929803073406, answer = # endangeredriver would be a sexy bastard in this channel if it had water. Quick turns. Narrow\n",
      "Span prob = 0.0021654858719557524, answer = The # endangeredriver would be a sexy bastard in this channel if it had water. Quick turns\n",
      "Span prob = 0.0020889979787170887, answer = The # endangeredriver would be a sexy bastard in this channel if it had water\n",
      "Span prob = 0.0018159415340051055, answer = # endangeredriver would be a sexy bastard in this channel if it had water. Quick turns\n",
      "Span prob = 0.0017517999513074756, answer = # endangeredriver would be a sexy bastard in this channel if it had water\n",
      "Span prob = 0.0015429999912157655, answer = endangeredriver would be a sexy bastard in this channel if it had water. Quick turns. Narrow. ( I'm losing it ) John D. Sutter\n",
      "Span prob = 0.0014155323151499033, answer = Quick turns. Narrow\n",
      "Span prob = 0.0013854490825906396, answer = endangered\n"
     ]
    }
   ],
   "source": [
    "def predict_answer(probs_start, probs_end, input_ids, tokenizer):\n",
    "    \n",
    "    input_length = len(input_ids)  # length of the input sequence, in the form \"[CLS] question [SEP] context\"\n",
    "\n",
    "    SEP_SPECIAL_TOKEN = 102  # the input id for the sep special token the separates the question from the context. The context starts after this token. \n",
    "    PAD_SPECIAL_TOKEN = 0  # the input id for padding tokens added to the end of the context\n",
    "    \n",
    "    span_probabilities = []  # save the probabilities here\n",
    "    spans = []  # save the possible answer spans here\n",
    "    \n",
    "    for start_idx in range(0, input_length):\n",
    "        for end_idx in range(0, input_length):\n",
    "            \n",
    "            start_prob = probs_start[start_idx]\n",
    "            end_prob = probs_end[end_idx]\n",
    "            \n",
    "            ### WRITE YOUR ANSWER HERE\n",
    "            \n",
    "            span_prob = start_prob * end_prob\n",
    "\n",
    "            span_probabilities.append(span_prob)\n",
    "            \n",
    "            span = tokenizer.decode(input_ids[start_idx:end_idx+1])\n",
    "            spans.append(span)\n",
    "\n",
    "    # sort the spans according to probability:\n",
    "    sorted_span_index = np.argsort(span_probabilities)\n",
    "    \n",
    "    # print the top 20 answers:\n",
    "    for i in range(20):\n",
    "        print(f'Span prob = {span_probabilities[sorted_span_index[-i-1]]}, answer = {spans[sorted_span_index[-i-1]]}')\n",
    "            \n",
    "predict_answer(probs_start[example_index], probs_end[example_index], example['input_ids'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db2d3b-9896-45dd-9c12-118e957bdb76",
   "metadata": {},
   "source": [
    "Are all of the top 20 valid and unique answers? If not, what do you think is going wrong? \n",
    "\n",
    "**TO-DO 5b:** Use the cell below to define a new and improved version of `predict_answer()` that only includes valid answers. Summarise in a couple of sentences what kind of invalid answers your code removes. **4 marks**\n",
    "\n",
    "WRITE YOUR ANSWER HERE:\n",
    "\n",
    "The updated function removes invalid answers that dont make sense in the context of answering the question. It discards answer spans that start after they end, spans that start or end on a special token such as [PAD], [SEP] or [CLS], and spans that start in the question part of the input sequence. This filtering helps to ensure that only potentially meaningful spans from the context are considered as valid answers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aae6c6d1-e3a4-4d3c-ba54-6e9e9b3d58bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span prob = 0.5619075298309326, answer = endangeredriver\n",
      "Span prob = 0.15243792533874512, answer = The # endangeredriver\n",
      "Span prob = 0.12783198058605194, answer = # endangeredriver\n",
      "Span prob = 0.007700338959693909, answer = endangeredriver would be a sexy bastard in this channel if it had water\n",
      "Span prob = 0.006627385504543781, answer = ##river\n",
      "Span prob = 0.0017517999513074756, answer = # endangeredriver would be a sexy bastard in this channel if it had water\n",
      "Span prob = 0.0014155323151499033, answer = Quick turns. Narrow\n",
      "Span prob = 0.0013854490825906396, answer = endangered\n",
      "Span prob = 0.0010366060305386782, answer = endangeredriver would be a sexy bastard\n",
      "Span prob = 0.000647262146230787, answer = endangeredriver would be a sexy bastard in this channel if it had water.\n",
      "Span prob = 0.0006158521864563227, answer = Quick turns\n",
      "Span prob = 0.000375853618606925, answer = The # endangered\n",
      "Span prob = 0.0003151847922708839, answer = # endangered\n",
      "Span prob = 0.00028121721697971225, answer = The # endangeredriver would be a sexy bastard\n",
      "Span prob = 0.0002502764400560409, answer = Narrow\n",
      "Span prob = 0.00023582420544698834, answer = # endangeredriver would be a sexy bastard\n",
      "Span prob = 0.00021596203441731632, answer = The #\n",
      "Span prob = 0.00021237862529233098, answer = endangeredriver would be a sexy bastard in this channel\n",
      "Span prob = 0.000190874605323188, answer = water. Quick turns. Narrow\n",
      "Span prob = 0.00018110228120349348, answer = #\n"
     ]
    }
   ],
   "source": [
    "def predict_answer(probs_start, probs_end, input_ids, tokenizer):\n",
    "    \n",
    "    input_length = len(input_ids)  # length of the input sequence, in the form \"[CLS] question [SEP] context\"\n",
    "\n",
    "    SEP_SPECIAL_TOKEN = 102  # the input id for the sep special token that separates the question from the context. The context starts after this token. \n",
    "    PAD_SPECIAL_TOKEN = 0  # the input id for padding tokens added to the end of the context\n",
    "    \n",
    "    span_probabilities = []  # save the probabilities here\n",
    "    spans = []  # save the possible answer spans here\n",
    "\n",
    "    # Find the position of [SEP] token so we don't include it in spans\n",
    "    sep_position = input_ids.index(SEP_SPECIAL_TOKEN)\n",
    "    \n",
    "    for start_idx in range(sep_position+1, input_length):  # start after the [SEP] token\n",
    "        for end_idx in range(start_idx, min(input_length, start_idx+15)):  # only consider spans up to length 15\n",
    "            \n",
    "            if input_ids[end_idx] == PAD_SPECIAL_TOKEN:  # break the loop when reaching a padding token\n",
    "                break\n",
    "\n",
    "            start_prob = probs_start[start_idx]\n",
    "            end_prob = probs_end[end_idx]\n",
    "            \n",
    "            # Only consider it if end index is greater or equal to start index\n",
    "            if end_idx >= start_idx:\n",
    "                span_prob = start_prob * end_prob\n",
    "                span = tokenizer.decode(input_ids[start_idx:end_idx+1])\n",
    "                span_probabilities.append(span_prob)\n",
    "                spans.append(span)\n",
    "\n",
    "    # sort the spans according to probability:\n",
    "    sorted_span_index = np.argsort(span_probabilities)\n",
    "    \n",
    "    # print the top 20 answers:\n",
    "    for i in range(min(20, len(span_probabilities))):\n",
    "        print(f'Span prob = {span_probabilities[sorted_span_index[-i-1]]}, answer = {spans[sorted_span_index[-i-1]]}')\n",
    "            \n",
    "predict_answer(probs_start[example_index], probs_end[example_index], example['input_ids'], tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ed4ee-3134-4f52-9c38-271830e0b1a3",
   "metadata": {},
   "source": [
    "You can try out the pretrained QA model on a few examples and try to identify its common mistakes.\n",
    "\n",
    "**TO-DO 5c:** State one way that we could improve the performance of our extractive QA model on the Tweet QA dataset.  **2 marks**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "One way in which we could imrpove the performance is to fine-tune it on a related task or dataset. The original dataset was initially trained on a SQuAD dataset that may not be optimal for understanding the nuances, slang etc in the tweets. Perhaps fin-tuning the model on a dataset that is closely related to the QA dataset could improve its perfomance. \n",
    "\n",
    "--- \n",
    "\n",
    "As well as answering ad-hoc queries, question answering models can help us to extract structured information about entities of interest from a large set of documents. Suppose that we want to automatically collect information on tech companies, such as Apple and Open AI. We want to extract information about each company's activities from social media, including the names and release dates of new products and services, the company's earnings in a specific year, and who its CEO is.  \n",
    "\n",
    "**TO-DO 5d:** Given a list of tech company names, how could we use question answering to extract the required information for each company from a set of tweets?  **(3 marks)** \n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "First we could filter the tweets that mention the tech companies. Then we create questions for eeach piece of information we want to extract which could be be anything based on that company. Then we use the QA model to answer each question using the relevant tweet as the context, if it contains the answer to the question, then we extract it. Finally, the extracted answers may need to be pre-processed to provide meaningful insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635306b-51de-4e9d-8608-8bd7473b294a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Transformer-based Text Classifiers (max. 24 marks)\n",
    "\n",
    "The previous section showed us how to use a pretrained QA model based on a pretrained transformer. In this section, you will learn how to construct and train a text classifier on top of a pretrained transformer. \n",
    "\n",
    "We will use the [Poem Sentiment](https://huggingface.co/datasets/poem_sentiment) dataset to train and test a classifier. The task is to classify lines from poems into one of  0: negative, 1: positive, 2: no impact, or 3: mixed sentiment. For more information, see [Sheng and Uthus, 2020](https://arxiv.org/pdf/2011.02686.pdf). \n",
    "\n",
    "To begin you will need to instantiate a suitable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1339d48-6225-48cd-b82c-306a89ad3342",
   "metadata": {},
   "source": [
    "**TO-DO 6a:** Find an AutoModel class that constructs a text classifier from the pretrained TinyBERT model, \"huawei-noah/TinyBERT_General_4L_312D\". Create the `model` object in the cell below using this class. Refer to the [Hugging Face documentation for auto models](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) as needed. **(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99068090-6a2d-47f0-9d22-b78a6705cd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.bias', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.0.weight', 'fit_denses.3.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'fit_denses.1.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.4.bias', 'cls.seq_relationship.bias', 'fit_denses.4.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR ANSWER TO 6a HERE ###\n",
    "\n",
    "# Importing relevant library\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Define the number of labels in the task (0: negative, 1: positive, 2: no impact, 3: mixed sentiment)\n",
    "num_labels = 4\n",
    "\n",
    "#instantiating my chosen model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a38d4e-27a1-4c0c-b3fa-e8dc3bdf61b5",
   "metadata": {},
   "source": [
    "**TO-DO 6b:** Provide a link to the documentation for your chosen auto model for text classification. Briefly describe how the text classifier `model` it creates differs from the QA model created by `AutoModelForQuestionAnswering`. Note: A useful reference may be the original BERT paper (https://arxiv.org/pdf/1810.04805.pdf), which includes diagrams (Figure 4) showing how BERT can be adapted to different tasks. **(2 marks)** \n",
    "\n",
    "WRITE YOUR ANSWER TO 6b HERE:\n",
    "\n",
    "Here is the link: https://huggingface.co/docs/transformers/model_doc/auto#automodelforsequenceclassification \n",
    "\n",
    "This model is used for tasks that involve classifying a sequence of text into one or more categories, whereas the QA model created by `AutoModelForQuestionAnswering` is used for tasks that involve finding the answer to a question in a given context. Basically, both models architecture is typically the same as the base transformer up until the final layer, where in my selected model the final layer is a linear layer that transforms the transformers output for the [CLS] token into logits for each of the classes. Compared to the QA model where instead of a single classification layer, there are two linear layers that are used to compute the start and end positions of the answer span within the context text. \n",
    "\n",
    "---\n",
    "\n",
    "For the QA task, the complete model was pretrained and we could apply it to a dataset without further training. However, for our poem sentiment classification task,\n",
    "we will need to train our model before we can use it (you may see a message in the output of the last cell telling you this). \n",
    "\n",
    "**TO-DO 6c:** The emotion classifier is built on top of a pretrained TinyBERT model, so why do we need to train it before we can use it? **(2 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER TO 6C HERE:\n",
    "\n",
    "We need to train it because it doesnt know anything about the specific task we're interested in, which is sentiment classification. The lower layers of the model, learned during pre-training can capture general language understanding, however the upper layers of the model need to be trained on our specific classification task to capture the task specific patterns.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Next, let's learn how to train our model. For some tasks it is not necessary to update the weights in the BERT model itself, so we can freeze them to save a lot of computation time. We can do this as follows. Since our pretrained model is based on BERT, we can access the weights inside BERT through the variable `model.bert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8ad56b1-d6e7-4048-9cc0-71849a003363",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0c065-fb40-4e24-b4e4-ffe9ed473ba9",
   "metadata": {},
   "source": [
    "To train our model, we can make use of the Trainer class, which encapsulates a lot of the complex training steps and avoids the need to define our own training function, as we did in the previous notebook (we don't need to write our own `train_nn`).\n",
    "\n",
    "First, define some settings for the training process. This is where we can set training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62af0a20-79a5-49ca-882a-638a68b288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"transformer_checkpoints\",  # specify the directory where models weights will be saved a certain points during training (checkpoints)\n",
    "    num_train_epochs=10, # A sensible and sufficient number to use for the to-dos below\n",
    "    per_device_train_batch_size=8,  # you can decrease this if memory usage is too high while training\n",
    "    logging_steps=50,  # how often to print progress during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77d14f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (./data_cache/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n",
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (./data_cache/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n"
     ]
    }
   ],
   "source": [
    "poem_train_dataset = load_dataset(\n",
    "    \"poem_sentiment\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "poem_val_dataset = load_dataset(\n",
    "    \"poem_sentiment\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b86a0-df7f-4978-a0ff-43d1b68975c5",
   "metadata": {},
   "source": [
    "Next, create a trainer object. Note that the next cell will currently fail with an error, because the variables `poem_train_dataset` and `poem_val_dataset` do not exist yet! Don't worry, we'll fix this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e5ba93c-f8c4-4542-a316-6aac4a32362d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "from torch import nn\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=poem_train_dataset,\n",
    "    eval_dataset=poem_val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33b30b-2c23-45e5-b2a1-757f1bf93ab7",
   "metadata": {},
   "source": [
    "To train the model, you will need to call `trainer.train()`.\n",
    "\n",
    "Once the model is trained, we can obtain predictions using the function below. Notice that it is simpler than obtaining the spans for QA -- we simply get the logits for each tweet in the test set, then apply argmax over the classes to find the most probable class for each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad4e689d-ebec-44c4-b81d-649079e4a4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_dataset):\n",
    "\n",
    "    # Switch off dropout\n",
    "    trained_model.eval()\n",
    "    \n",
    "    # Pass the required items from the dataset to the model    \n",
    "    output = trained_model(attention_mask=torch.tensor(test_dataset[\"attention_mask\"]), input_ids=torch.tensor(test_dataset[\"input_ids\"]))\n",
    "                        \n",
    "    # the output dictionary contains logits, which are the unnormalised scores for each class for each example:\n",
    "    pred_labs = np.argmax(output[\"logits\"].detach().numpy(), axis=1)\n",
    "\n",
    "    return pred_labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf8564-4f98-4e3f-b1f7-b59725a0605c",
   "metadata": {},
   "source": [
    "You should now have all the bits and pieces needed to build and train a text classifier. Let's put them all together...\n",
    "\n",
    "**TO-DO 6d:** Implement and test a classifier for the [Poem Sentiment](https://huggingface.co/datasets/poem_sentiment) dataset using a pretrained transformer. Evaluate the classifier with both frozen and unfrozen (i.e., fine-tuned) parameters in the pretrained transformer. Choose a suitable evaluation metric and provide a comparison of the results below, including a brief explanation  (1-2 sentences) for any differences you observe between the frozen and unfrozen variants. Make sure to comment your code.  **(10 marks)**\n",
    "\n",
    "Notes: \n",
    " * Strong classifier performance is not required to achive good marks -- rather, we award marks for implementing and testing a transformer-based classifier correctly.\n",
    " * You may implement any suitable kind of classifier you like, as long as you are using a pretrained transformer model.\n",
    " * 'tiny' BERT variants such as TinyBERT and roberta-tiny are recommended because they are small enough to fine-tune with a typical laptop CPU. We recommend sticking with these smaller pretrained models unless you have access to a GPU, e.g., via Google Colab. \n",
    "\n",
    "WRITE YOUR ANSWER HERE (DESCRIPTION OF RESULTS FOR 6d):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fcd5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0740b87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (/Users/thomascourts/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n",
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (/Users/thomascourts/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df8955b4e3f4f7e9c07a3c617cfab7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555b91c08e2743e1b039e990a6ed58f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "poem_train_dataset = load_dataset(\n",
    "    \"poem_sentiment\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "poem_val_dataset = load_dataset(\n",
    "    \"poem_sentiment\",\n",
    "    split=\"validation\",\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['verse_text'], padding=True, truncation=True)\n",
    "\n",
    "poem_train_dataset = poem_train_dataset.map(tokenize, batched=True, batch_size=len(poem_train_dataset))\n",
    "poem_val_dataset = poem_val_dataset.map(tokenize, batched=True, batch_size=len(poem_val_dataset))\n",
    "\n",
    "poem_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "poem_val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6fa1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate performance metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7a5bd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.bias', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.0.weight', 'fit_denses.3.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'fit_denses.1.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.4.bias', 'cls.seq_relationship.bias', 'fit_denses.4.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=4)\n",
    "\n",
    "# Freeze the BERT parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76e58137",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 21\u001b[0m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     14\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mpoem_val_dataset,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/transformers/trainer.py:1909\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1909\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1910\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/arrow_dataset.py:1925\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/arrow_dataset.py:1910\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   1908\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, decoded\u001b[38;5;241m=\u001b[39mdecoded, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   1909\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1910\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:540\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     pa_table_to_format \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mdrop(col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m format_columns)\n\u001b[0;32m--> 540\u001b[0m     formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table_to_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_all_columns:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_output, MutableMapping):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:281\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/torch_formatter.py:57\u001b[0m, in \u001b[0;36mTorchFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(row)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:154\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:160\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arrow_array_to_numpy(pa_table[col]) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:160\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arrow_array_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:196\u001b[0m, in \u001b[0;36mNumpyArrowExtractor._arrow_array_to_numpy\u001b[0;34m(self, pa_array)\u001b[0m\n\u001b[1;32m    194\u001b[0m         array: List \u001b[38;5;241m=\u001b[39m pa_array\u001b[38;5;241m.\u001b[39mto_numpy(zero_copy_only\u001b[38;5;241m=\u001b[39mzero_copy_only)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_array_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mobject})\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_array_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:197\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m         array: List \u001b[38;5;241m=\u001b[39m pa_array\u001b[38;5;241m.\u001b[39mto_numpy(zero_copy_only\u001b[38;5;241m=\u001b[39mzero_copy_only)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m--> 197\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m (x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m array[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(x))\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m array\n\u001b[1;32m    200\u001b[0m     ):\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_array_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mobject})\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_array_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=poem_train_dataset,\n",
    "    eval_dataset=poem_val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0bd1936",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/transformers/trainer.py:1909\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1909\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1910\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/arrow_dataset.py:1925\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/arrow_dataset.py:1910\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   1908\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, decoded\u001b[38;5;241m=\u001b[39mdecoded, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   1909\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1910\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:540\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     pa_table_to_format \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mdrop(col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m format_columns)\n\u001b[0;32m--> 540\u001b[0m     formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table_to_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_all_columns:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_output, MutableMapping):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:281\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/torch_formatter.py:57\u001b[0m, in \u001b[0;36mTorchFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(row)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:154\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:160\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arrow_array_to_numpy(pa_table[col]) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:160\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arrow_array_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:196\u001b[0m, in \u001b[0;36mNumpyArrowExtractor._arrow_array_to_numpy\u001b[0;34m(self, pa_array)\u001b[0m\n\u001b[1;32m    194\u001b[0m         array: List \u001b[38;5;241m=\u001b[39m pa_array\u001b[38;5;241m.\u001b[39mto_numpy(zero_copy_only\u001b[38;5;241m=\u001b[39mzero_copy_only)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_array_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mobject})\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_array_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/datasets/formatting/formatting.py:197\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m         array: List \u001b[38;5;241m=\u001b[39m pa_array\u001b[38;5;241m.\u001b[39mto_numpy(zero_copy_only\u001b[38;5;241m=\u001b[39mzero_copy_only)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m--> 197\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m (x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m array[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(x))\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m array\n\u001b[1;32m    200\u001b[0m     ):\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_array_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mobject})\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_array_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_analytics/lib/python3.9/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# Unfreeze the BERT parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cffd2d",
   "metadata": {},
   "source": [
    "Unfortunately I was unable to get the code to work, however I did give it an attempt and got quite close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1661d-a203-42ee-8681-025f2bdfe661",
   "metadata": {},
   "source": [
    "**TO-DO 6e:** Did your sentiment classifier make use of any kind of model transfer or transfer learning? If so, what kinds of transfer were used and what benefit do they provide? **(4 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE:\n",
    "\n",
    "Yes, my sentiment classifier made use of transfer learning. It utilised a pretrained transformer model called TinyBERT which was trained on a large corpus of text from a different task. The transfer learning involved transferring the knowledge gained by the transformer during its pretraining phase to the sentiment classification task. By using this pretrained model the sentiment classifier benefits from general language understanding learned by the transformer\n",
    "model. Transfer learning with pretrained models helps improve the performance of the sentiment classifier by enabling it to learn more effectively from a smaller labelled dataset. The transfer of knowledge from the pretrained model helps the sentiment classifier generalise better and capture relevant patterns and achieve higher accuracy compared to training the classifier from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "**TO-DO 6f:** Use your model to compute the probability of sentiment for a sentence of your choosing. Comment your code and print the sentence with its probability distribution. Label the values so that we know which class they refer to. **(4 marks)**\n",
    "\n",
    "Hint: you could use a poem generator, such as [this one](https://www.poemofquotes.com/tools/poetry-generator/ai-poem-generator), to generate a test sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11fcdc69-ce30-4eb3-afc5-4eeba41b40db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sentence:  The sun sets on the horizon, painting the sky in hues of gold and crimson.\n",
      "Probability Distribution:\n",
      "Negative: 24.89%\n",
      "Positive: 25.09%\n",
      "No Impact: 24.92%\n",
      "Mixed Sentiment: 25.10%\n"
     ]
    }
   ],
   "source": [
    "# Define the sentence for which we want to compute the sentiment probability \n",
    "test_sentence = \"The sun sets on the horizon, painting the sky in hues of gold and crimson.\"\n",
    "\n",
    "\n",
    "# Tokenize the test sentence\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "tokens = tokenizer.encode_plus(test_sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensors to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_ids = tokens[\"input_ids\"].to(device)\n",
    "attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Compute the logits (unnormalized scores) for each sentiment class\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Compute the softmax probabilities for each sentiment class\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "# Get the sentiment labels and their corresponding probabilities\n",
    "sentiment_labels = [\"Negative\", \"Positive\", \"No Impact\", \"Mixed Sentiment\"]\n",
    "probabilities = probs.squeeze().tolist()\n",
    "\n",
    "# Print the test sentence and its probability distribution\n",
    "print(\"Test Sentence: \", test_sentence)\n",
    "print(\"Probability Distribution:\")\n",
    "for label, prob in zip(sentiment_labels, probabilities):\n",
    "    print(label + \": {:.2f}%\".format(prob * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb0d80-4624-4690-a94d-b9f3c1a60d7f",
   "metadata": {},
   "source": [
    "# 7. OPTIONAL: More on Transformers\n",
    "\n",
    "There are many great resources out there to show you how to use this kind of model in practice:\n",
    "* An extensive online course is provided by HuggingFace: https://huggingface.co/course/chapter1/1. The pages linked from the HuggingFace course website have an 'open in Colab' button on the top right. You can open the notebook and run it on a Google server there to access GPUs.\n",
    "* Chapters that may be particularly useful: \n",
    "   * Transformers, what can they do? https://huggingface.co/course/chapter1/3?fw=pt\n",
    "   * Using Transformers: https://huggingface.co/course/chapter2/2?fw=pt\n",
    "* They provide information on fine-tuning the transformer models here: https://huggingface.co/docs/transformers/training. Fine-tuning updates the weights inside the pretrained network and requires extensive GPU or TPU computing. \n",
    "* Text Generation: https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb. This topic goes way beyond data analytics on this unit and shows you another powerful feature of pretrained transformers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e18f91-4107-437a-b939-caf4c321e7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
